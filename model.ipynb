{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf52088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Visual Code\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments,\n",
    "                          Seq2SeqTrainer, DataCollatorForSeq2Seq)\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b34df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.backends import cuda\n",
    "\n",
    "cuda.matmul.allow_tf32 = True\n",
    "cuda.enable_flash_sdp(True)\n",
    "cuda.enable_math_sdp(True)\n",
    "cuda.enable_mem_efficient_sdp(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f6da0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since xsum couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\Ignke\\.cache\\huggingface\\datasets\\xsum\\default\\1.2.0\\082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71 (last modified on Wed Dec  3 03:57:12 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test sizes: 38000/1000/1000\n"
     ]
    }
   ],
   "source": [
    "# Load datasets (only the 'train' splits, as we'll split manually)\n",
    "cnn = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
    "xsum = load_dataset(\"xsum\", split=\"train\")\n",
    "\n",
    "# Take a subset of 1000 each\n",
    "cnn_small = cnn.select(range(20000))\n",
    "xsum_small = xsum.select(range(20000))\n",
    "\n",
    "# Add prefix and rename fields to unify format\n",
    "def add_prefix_cnn(example):\n",
    "    return {\n",
    "        \"text\": \"detailed: \" + example[\"article\"], \n",
    "        \"summary\": example[\"highlights\"]\n",
    "    }\n",
    "def add_prefix_xsum(example):\n",
    "    return {\n",
    "        \"text\": \"extreme: \" + example[\"document\"], \n",
    "        \"summary\": example[\"summary\"]\n",
    "    }\n",
    "\n",
    "cnn_pfx = cnn_small.map(add_prefix_cnn, remove_columns=cnn_small.column_names)\n",
    "xsum_pfx = xsum_small.map(add_prefix_xsum, remove_columns=xsum_small.column_names)\n",
    "\n",
    "# Combine and split (train:80%, val:10%, test:10%)\n",
    "dataset = concatenate_datasets([cnn_pfx, xsum_pfx])\n",
    "split1 = dataset.train_test_split(test_size=0.05, seed=42)  # 9500 train, 500 temp\n",
    "split2 = split1['test'].train_test_split(test_size=0.50, seed=42)  # 250 val, 250 test\n",
    "\n",
    "train_dataset = split1['train']\n",
    "val_dataset = split2['train']\n",
    "test_dataset = split2['test']\n",
    "print(f\"Train/Val/Test sizes: {len(train_dataset)}/{len(val_dataset)}/{len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "596ffe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "max_source_length = 512\n",
    "max_target_length = 160\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs = tokenizer(batch[\"text\"], max_length=max_source_length, truncation=True, padding=\"max_length\")\n",
    "    # Tokenize targets with the `text_target` argument (T5)\n",
    "    labels = tokenizer(text_target=batch[\"summary\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "    # Replace pad token id's in labels by -100 so they are ignored in loss\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(tok if tok != tokenizer.pad_token_id else -100) for tok in label] \n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_train = train_dataset.map(preprocess, batched=True, remove_columns=[\"text\", \"summary\"])\n",
    "tokenized_val   = val_dataset.map(preprocess, batched=True, remove_columns=[\"text\", \"summary\"])\n",
    "tokenized_test  = test_dataset.map(preprocess, batched=True, remove_columns=[\"text\", \"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e444cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"google/flan-t5-large\",\n",
    "    torch_dtype=torch.bfloat16,  # IMPORTANT for speed\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# activates xformers / memory-efficient attention where possible\n",
    "model = model.to(\"cuda\")\n",
    "model = model.to(memory_format=torch.contiguous_format)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46e70ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,386,304 || all params: 795,536,384 || trainable%: 1.5570\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"SelfAttention.q\",\n",
    "        \"SelfAttention.k\",\n",
    "        \"SelfAttention.v\",\n",
    "        \"SelfAttention.o\",\n",
    "        \"EncDecAttention.q\",\n",
    "        \"EncDecAttention.k\",\n",
    "        \"EncDecAttention.v\",\n",
    "        \"EncDecAttention.o\",\n",
    "        \"DenseReluDense.wi\",\n",
    "        \"DenseReluDense.wo\"\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model.print_trainable_parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f37374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan-t5-summ-peft\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=200,\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    label_smoothing_factor=0.0,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c1199f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    \n",
    "    # preds may be (generated_ids, scores)\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = np.asarray(preds)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    # Replace all negative values (including -100)\n",
    "    preds = np.where(preds < 0, tokenizer.pad_token_id, preds)\n",
    "    labels = np.where(labels < 0, tokenizer.pad_token_id, labels)\n",
    "\n",
    "    # Convert to lists\n",
    "    preds_list = preds.tolist()\n",
    "    labels_list = labels.tolist()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(\n",
    "        preds_list,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    decoded_labels = tokenizer.batch_decode(\n",
    "        labels_list,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    decoded_preds = [p.strip() for p in decoded_preds]\n",
    "    decoded_labels = [l.strip() for l in decoded_labels]\n",
    "\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "\n",
    "    out = {}\n",
    "    for k, v in result.items():\n",
    "        if hasattr(v, \"mid\"):\n",
    "            out[k] = v.mid.fmeasure * 100\n",
    "        else:\n",
    "            out[k] = v * 100\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": round(out.get(\"rouge1\", 0), 4),\n",
    "        \"rouge2\": round(out.get(\"rouge2\", 0), 4),\n",
    "        \"rougeL\": round(out.get(\"rougeL\", 0), 4),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe14cd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ignke\\AppData\\Local\\Temp\\ipykernel_24220\\2358971432.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2375' max='2375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2375/2375 5:14:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.736300</td>\n",
       "      <td>1.528251</td>\n",
       "      <td>38.548000</td>\n",
       "      <td>15.989400</td>\n",
       "      <td>29.446100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('flan_t5_detailed_extreme\\\\tokenizer_config.json',\n",
       " 'flan_t5_detailed_extreme\\\\special_tokens_map.json',\n",
       " 'flan_t5_detailed_extreme\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "model.to(torch.bfloat16)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"flan_t5_detailed_extreme\")\n",
    "tokenizer.save_pretrained(\"flan_t5_detailed_extreme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88f51b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('flan_t5_detailed_extreme\\\\tokenizer_config.json',\n",
       " 'flan_t5_detailed_extreme\\\\special_tokens_map.json',\n",
       " 'flan_t5_detailed_extreme\\\\tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"flan_t5_detailed_extreme\")\n",
    "tokenizer.save_pretrained(\"flan_t5_detailed_extreme\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a0ad733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article words:  480\n",
      "Detailed summary: The gaming industry has grown from a niche hobby into a global cultural and economic powerhouse . Advances in graphics, processing power, and internet connectivity have enabled increasingly sophisticated and visually stunning games . Mobile technology has expanded gaming’s reach, breaking down traditional barriers of access . Social features have turned gaming into a shared cultural experience .\n",
      "Extreme summary: The gaming industry has grown from a niche hobby into a global cultural and economic powerhouse.\n",
      "Detailed summary words:  58\n",
      "Extreme summary words:  16\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"flan_t5_detailed_extreme\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"flan_t5_detailed_extreme\")\n",
    "\n",
    "def summarize(text, style=\"detailed\"):\n",
    "    prefix = \"detailed: \" if style==\"detailed\" else \"extreme: \"\n",
    "    input_text = prefix + text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=max_source_length).to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage:\n",
    "article = (\"\"\"\n",
    "           The gaming industry has transformed dramatically over the past few decades, evolving from a niche hobby into a global cultural and economic powerhouse. What began as simple pixelated games on arcade machines and early home consoles has grown into a complex ecosystem encompassing PC, console, and mobile platforms, competitive esports, and immersive virtual realities. Its influence now extends beyond entertainment, affecting technology, culture, and even social interaction.\n",
    "\n",
    "One key factor behind the gaming industry’s growth is technological innovation. Advances in graphics, processing power, and internet connectivity have enabled increasingly sophisticated and visually stunning games. Open-world environments, realistic physics, and dynamic storytelling immerse players in experiences that were unimaginable just a decade ago. Mobile technology has expanded gaming’s reach, allowing millions to play anywhere, anytime, breaking down traditional barriers of access and making gaming a truly global phenomenon.\n",
    "\n",
    "The rise of online multiplayer and social gaming has also reshaped how people interact with games. Platforms like Xbox Live, PlayStation Network, and Steam allow players to connect across continents, forming communities and competing in real-time. Social features—such as streaming on platforms like Twitch or sharing gameplay on YouTube—have turned gaming into a shared cultural experience. Competitive gaming, or esports, has grown into a billion-dollar industry with professional players, sponsored tournaments, and dedicated fanbases rivaling traditional sports.\n",
    "\n",
    "Economic growth in the gaming sector has been staggering. Global revenue now surpasses that of the film and music industries combined, driven by digital distribution, in-game purchases, and subscription models. Game development studios range from small indie teams creating niche titles to multinational corporations producing blockbuster franchises. This diversity fosters creativity and innovation, allowing unique experiences to reach audiences that were once considered too small to support niche games.\n",
    "\n",
    "However, the industry faces challenges alongside its growth. Concerns over game addiction, online harassment, and the monetization of microtransactions have sparked debates about regulation and ethical practices. Additionally, the rapid pace of technological change pressures developers to continually innovate, while cybersecurity threats pose risks for both companies and players. Balancing profitability, player engagement, and ethical responsibility is a central challenge for the modern gaming industry.\n",
    "\n",
    "Looking forward, emerging technologies like virtual reality (VR), augmented reality (AR), and cloud gaming promise to redefine the gaming experience. VR immerses players fully in digital worlds, while AR blends real and virtual environments for interactive experiences. Cloud gaming enables high-quality gameplay without expensive hardware, further democratizing access. These innovations suggest that gaming will continue to expand its influence, blurring the lines between entertainment, education, and social connection.\n",
    "\n",
    "In conclusion, the gaming industry has grown from a small pastime into a major economic, cultural, and technological force. Its success stems from innovation, accessibility, and community-driven experiences, while its challenges highlight the need for responsible development and ethical consideration. As technology continues to evolve, gaming is poised to play an even larger role in shaping how people interact, learn, and entertain themselves worldwide.\n",
    "           \"\"\")\n",
    "\n",
    "print(\"Article words: \", len(article.split()))\n",
    "print(\"Detailed summary:\", summarize(article, style=\"detailed\"))\n",
    "print(\"Extreme summary:\", summarize(article, style=\"extreme\"))\n",
    "print(\"Detailed summary words: \", len(summarize(article, style=\"detailed\").split()))\n",
    "print(\"Extreme summary words: \", len(summarize(article, style=\"extreme\").split()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
