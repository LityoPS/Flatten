{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "307ab79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall -y datasets > /dev/null 2>&1 && pip install datasets==3.6.0 > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af460758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from datasets.utils.logging import set_verbosity_error\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer,T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec048e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAMES = [\"cnn_dailymail\", \"xsum\"]\n",
    "VERSION = \"3.0.0\"\n",
    "\n",
    "set_verbosity_error()\n",
    "\n",
    "xsum = load_dataset(DATASET_NAMES[1], trust_remote_code=True)\n",
    "cnn_dailymail = load_dataset(DATASET_NAMES[0], VERSION)\n",
    "\n",
    "xsum = xsum.remove_columns(['id'])\n",
    "xsum = xsum.rename_columns({\n",
    "    \"document\" : \"text\",\n",
    "    \"summary\" : \"summary\"\n",
    "})\n",
    "\n",
    "cnn_dailymail = cnn_dailymail.remove_columns(['id'])\n",
    "cnn_dailymail = cnn_dailymail.rename_columns({\n",
    "    \"article\" : \"text\",\n",
    "    \"highlights\" : \"summary\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c76bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'summary'],\n",
      "    num_rows: 491158\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'summary'],\n",
      "    num_rows: 24700\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'summary'],\n",
      "    num_rows: 22824\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SAMPLES = 5000\n",
    "VAL_SAMPLES = 500\n",
    "TEST_SAMPLES = 500\n",
    "\n",
    "df_train_xsum = xsum[\"train\"].to_pandas()\n",
    "df_validation_xsum = xsum[\"validation\"].to_pandas()\n",
    "df_test_xsum =xsum[\"test\"].to_pandas()\n",
    "\n",
    "df_train_cnn = cnn_dailymail[\"train\"].to_pandas()\n",
    "df_validation_cnn = cnn_dailymail[\"validation\"].to_pandas()\n",
    "df_test_cnn =cnn_dailymail[\"test\"].to_pandas()\n",
    "\n",
    "df_train_xsum = df_train_xsum.sample(n=TRAIN_SAMPLES//2, random_state=42)\n",
    "df_validation_xsum = df_validation_xsum.sample(n=VAL_SAMPLES//2, random_state=42)\n",
    "df_test_xsum = df_test_xsum.sample(n=TEST_SAMPLES//2, random_state=42)\n",
    "\n",
    "df_train_cnn = df_train_cnn.sample(n=TRAIN_SAMPLES//2, random_state=42)\n",
    "df_validation_cnn = df_validation_cnn.sample(n=VAL_SAMPLES//2, random_state=42)\n",
    "df_test_cnn = df_test_cnn.sample(n=TEST_SAMPLES//2, random_state=42)\n",
    "\n",
    "df_train = pd.concat([df_train_xsum, df_train_cnn])\n",
    "df_validation = pd.concat([df_validation_xsum, df_validation_cnn])\n",
    "df_test = pd.concat([df_test_xsum, df_test_cnn])\n",
    "\n",
    "print(f\"Train Size: {df_train.shape}\")\n",
    "print(f\"Validation Size: {df_validation.shape}\")\n",
    "print(f\"Test Size: {df_test.shape}\")\n",
    "\n",
    "# train_dataset = concatenate_datasets([xsum[\"train\"], cnn_dailymail[\"train\"]])\n",
    "# validation_dataset = concatenate_datasets([xsum[\"validation\"], cnn_dailymail[\"validation\"]])\n",
    "# test_dataset = concatenate_datasets([xsum[\"test\"], cnn_dailymail[\"test\"]])\n",
    "\n",
    "# print(train_dataset)\n",
    "# print(validation_dataset)\n",
    "# print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff9a541",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189fe8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    MODEL_NAME = \"t5-base\"\n",
    "    MAX_INPUT_LENGTH = 512\n",
    "    MAX_TARGET_LENGTH = 128\n",
    "    BATCH_SIZE = 8\n",
    "    EPOCHS = 3\n",
    "    LEARNING_RATE = 3e-4\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    WARMUP_STEPS = 500\n",
    "    OUTPUT_DIR = \"./t5_summarization_model\"\n",
    "    LOGGING_STEPS = 100\n",
    "    SAVE_STEPS = 1000\n",
    "    EVAL_STEPS = 1000\n",
    "    HARSH_THRESHOLD = 0.30\n",
    "    STANDARD_THRESHOLD = 0.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_compression_ratio(text, summary):\n",
    "    text_len = len(text.split())\n",
    "    summary_len = len(summary.split())\n",
    "    if text_len == 0:\n",
    "        return 0\n",
    "    return summary_len / text_len\n",
    "\n",
    "def assign_length_category(ratio):\n",
    "    if ratio < Config.HARSH_THRESHOLD:\n",
    "        return \"harsh\"\n",
    "    elif ratio < Config.STANDARD_THRESHOLD:\n",
    "        return \"standard\"\n",
    "    else:\n",
    "        return \"detailed\"\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df['compression_ratio'] = df.apply(lambda row: calculate_compression_ratio(row['text'], row['summary']), axis=1)\n",
    "    df['length_category'] = df['compression_ratio'].apply(assign_length_category)\n",
    "    df['input_text'] = df.apply(lambda row: f\"summarize {row['length_category']}: {row['text']}\", axis=1)\n",
    "    df['target_text'] = df['summary']\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c485cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_input_length, max_target_length):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        input_encoding = self.tokenizer(\n",
    "            row['input_text'],\n",
    "            max_length=self.max_input_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        target_encoding = self.tokenizer(\n",
    "            row['target_text'],\n",
    "            max_length=self.max_target_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        labels = target_encoding['input_ids']\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': input_encoding['attention_mask'].squeeze(),\n",
    "            'labels': labels.squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421edcf0",
   "metadata": {},
   "source": [
    "## Model Training & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f107dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer():\n",
    "    model = T5ForConditionalGeneration.from_pretrained(Config.MODEL_NAME)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ab610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df_train, df_val, df_test, text_col='text', summary_col='summary'):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model, tokenizer = setup_model_and_tokenizer()\n",
    "    model.to(device)\n",
    "    \n",
    "    df_train = preprocess_df(df_train)\n",
    "    df_val = preprocess_df(df_validation)\n",
    "    df_test = preprocess_df(df_test)\n",
    "        \n",
    "    train_dataset = SummarizationDataset(df_train, tokenizer, Config.MAX_INPUT_LENGTH, Config.MAX_TARGET_LENGTH)\n",
    "    validation_dataset = SummarizationDataset(df_val, tokenizer, Config.MAX_INPUT_LENGTH, Config.MAX_TARGET_LENGTH)\n",
    "    test_dataset = SummarizationDataset(df_test, tokenizer, Config.MAX_INPUT_LENGTH, Config.MAX_TARGET_LENGTH)\n",
    "    \n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=Config.OUTPUT_DIR,\n",
    "        num_train_epochs=Config.EPOCHS,\n",
    "        per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=Config.BATCH_SIZE,\n",
    "        learning_rate=Config.LEARNING_RATE,\n",
    "        weight_decay=Config.WEIGHT_DECAY,\n",
    "        warmup_steps=Config.WARMUP_STEPS,\n",
    "        logging_steps=Config.LOGGING_STEPS,\n",
    "        eval_steps=Config.EVAL_STEPS,\n",
    "        save_steps=Config.SAVE_STEPS,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\nSaving final model...\")\n",
    "    trainer.save_model(Config.OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(Config.OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\nModel saved to {Config.OUTPUT_DIR}\")\n",
    "    \n",
    "    return model, tokenizer, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text, model, tokenizer, length_style=\"standard\", max_length=150, device='cuda'):\n",
    "    model.eval()\n",
    "    \n",
    "    input_text = f\"summarize {length_style}: {text}\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        max_length=Config.MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    \n",
    "    if length_style == \"harsh\":\n",
    "        max_gen_length = int(max_length * 0.6)\n",
    "    elif length_style == \"standard\":\n",
    "        max_gen_length = max_length\n",
    "    else:\n",
    "        max_gen_length = int(max_length * 1.4)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=max_gen_length,\n",
    "            num_beams=4,\n",
    "            length_penalty=1.0,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "    \n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c2a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Creating dummy data for demonstration...\")\n",
    "\n",
    "    model, tokenizer, trainer = train_model(\n",
    "        df_train, df_validation, df_test,\n",
    "        text_col='text',\n",
    "        summary_col='summary'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Example Inference\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    sample_text = \"Artificial intelligence is transforming the world. Machine learning models are becoming more powerful every day.\"\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for style in [\"harsh\", \"standard\", \"detailed\"]:\n",
    "        summary = generate_summary(sample_text, model, tokenizer, style, device=device)\n",
    "        print(f\"{style.upper()}: {summary}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
